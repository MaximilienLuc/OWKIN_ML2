{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:49.282534Z",
     "start_time": "2022-10-24T07:22:35.359325Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n",
    "!pip install scikit-learn\n",
    "!pip install torch\n",
    "!pip install xgboost --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:50.448803Z",
     "start_time": "2022-10-24T07:22:49.285830Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with own direction\n",
    "LOCAL_REPOSITORY_DIR = \"/Users/enfants/Code/OWKIN_ML2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "your_data_dir/\n",
    "├── train_output.csv\n",
    "├── train_input/\n",
    "│   ├── images/\n",
    "│       ├── ID_001/\n",
    "│           ├── ID_001_tile_000_17_170_43.jpg\n",
    "...\n",
    "│   └── moco_features/\n",
    "│       ├── ID_001.npy\n",
    "...\n",
    "├── test_input/\n",
    "│   ├── images/\n",
    "│       ├── ID_003/\n",
    "│           ├── ID_003_tile_000_16_114_93.jpg\n",
    "...\n",
    "│   └── moco_features/\n",
    "│       ├── ID_003.npy\n",
    "...\n",
    "├── supplementary_data/\n",
    "│   ├── baseline.ipynb\n",
    "│   ├── test_metadata.csv\n",
    "│   └── train_metadata.csv\n",
    "```\n",
    "\n",
    "For instance, `your_data_dir = /storage/DATA_CHALLENGE_ENS_2022/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put your own path to the data root directory (see example in `Data architecture` section)\n",
    "data_dir = Path(LOCAL_REPOSITORY_DIR)\n",
    "\n",
    "# load the training and testing data sets\n",
    "train_features_dir = data_dir / \"train_input\" / \"moco_features\"\n",
    "test_features_dir = data_dir / \"test_input\" / \"moco_features\"\n",
    "df_train = pd.read_csv(data_dir  / \"supplementary_data\" / \"train_metadata.csv\")\n",
    "df_test = pd.read_csv(data_dir  / \"supplementary_data\" / \"test_metadata.csv\")\n",
    "\n",
    "# concatenate y_train and df_train\n",
    "y_train = pd.read_csv(data_dir  / \"train_output.csv\")\n",
    "df_train = df_train.merge(y_train, on=\"Sample ID\")\n",
    "\n",
    "print(f\"Training data dimensions: {df_train.shape}\")  # (344, 4)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "centers_train = []\n",
    "patients_train = []\n",
    "samples_train = []\n",
    "\n",
    "for sample, label, center, patient in tqdm(  # loading bar\n",
    "    df_train[[\"Sample ID\", \"Target\", \"Center ID\", \"Patient ID\"]].values\n",
    "):\n",
    "    # load the coordinates and features (1000, 3+2048)\n",
    "    _features = np.load(train_features_dir / sample)\n",
    "    # split coords / features\n",
    "    coordinates, features = _features[:, :3], _features[:, 3:]  # Ks\n",
    "\n",
    "    # append each tile as a training sample\n",
    "    X_train.append(features)\n",
    "    y_train.append(np.full(len(features), label))\n",
    "    centers_train.append(np.full(len(features), center))\n",
    "    patients_train.append(np.full(len(features), patient))\n",
    "    samples_train.append(np.full(len(features), sample))\n",
    "\n",
    "# convert to numpy arrays\n",
    "X_train = np.vstack(X_train)          # (N_tiles, 2048)\n",
    "y_train = np.concatenate(y_train)     # (N_tiles,)\n",
    "centers_train = np.concatenate(centers_train)\n",
    "patients_train = np.concatenate(patients_train)\n",
    "samples_train = np.concatenate(samples_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global configuration\n",
    "CV_OUTER_SPLITS = 5\n",
    "CV_OUTER_REPEATS = 2\n",
    "CV_INNER_SPLITS = 3\n",
    "MAX_TILES_PER_PATIENT = 150\n",
    "AGG_CANDIDATES = [\"mean\", \"median\", \"topk_mean\"]\n",
    "TOPK_FRAC = 0.10\n",
    "ROBUST_SCORE_WEIGHTS = {\"global_auc\": 0.8, \"center_min_auc\": 0.2}\n",
    "SELECTED_MODEL_NAME = \"rf\"  # options: \"lr\", \"hgb\", \"rf\"\n",
    "\n",
    "\n",
    "def set_global_seed(seed):\n",
    "    import random\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    try:\n",
    "        import torch\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        if hasattr(torch.backends, \"cudnn\"):\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "    except Exception:\n",
    "        # torch may be unavailable at this stage\n",
    "        pass\n",
    "\n",
    "\n",
    "set_global_seed(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /!\\ we perform splits at the patient level so that all samples from the same patient\n",
    "# are found in the same split\n",
    "\n",
    "patients_unique = np.unique(patients_train)\n",
    "y_unique = np.array(\n",
    "    [np.mean(y_train[patients_train == p]) for p in patients_unique]\n",
    ")\n",
    "centers_unique = np.array(\n",
    "    [centers_train[patients_train == p][0] for p in patients_unique]\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"Training set specifications\\n\"\n",
    "    \"---------------------------\\n\"\n",
    "    f\"{len(X_train)} unique samples\\n\"\n",
    "    f\"{len(patients_unique)} unique patients\\n\"\n",
    "    f\"{len(np.unique(centers_unique))} unique centers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple patient-level tile sampling (XX tiles / patient)\n",
    "\n",
    "rng = np.random.RandomState(GLOBAL_SEED)\n",
    "\n",
    "keep_idx = []\n",
    "meta = []\n",
    "\n",
    "for p in np.unique(patients_train):\n",
    "    idx = np.where(patients_train == p)[0]\n",
    "\n",
    "    if len(idx) > MAX_TILES_PER_PATIENT:\n",
    "        idx = rng.choice(idx, MAX_TILES_PER_PATIENT, replace=False)\n",
    "\n",
    "    keep_idx.append(idx)\n",
    "    meta.append({\n",
    "        \"patient\": p,\n",
    "        \"n_tiles_before\": np.sum(patients_train == p),\n",
    "        \"n_tiles_after\": len(idx),\n",
    "        \"label\": y_train[idx][0],\n",
    "    })\n",
    "\n",
    "keep_idx = np.concatenate(keep_idx)\n",
    "\n",
    "# apply sampling\n",
    "X_train = X_train[keep_idx]\n",
    "y_train = y_train[keep_idx]\n",
    "centers_train = centers_train[keep_idx]\n",
    "patients_train = patients_train[keep_idx]\n",
    "samples_train = samples_train[keep_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(model, X, y, patients, n_splits=5):\n",
    "    \n",
    "    patients_unique = np.unique(patients)\n",
    "    y_unique = np.array([y[patients == p][0] for p in patients_unique])\n",
    "\n",
    "    aucs = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_pat_idx, val_pat_idx) in enumerate(\n",
    "        skf.split(patients_unique, y_unique)\n",
    "    ):\n",
    "        train_patients = patients_unique[train_pat_idx]\n",
    "        val_patients = patients_unique[val_pat_idx]\n",
    "\n",
    "        train_idx = np.where(pd.Series(patients).isin(train_patients))[0]\n",
    "        val_idx = np.where(pd.Series(patients).isin(val_patients))[0]\n",
    "\n",
    "        X_train_fold, y_train_fold = X[train_idx], y[train_idx]\n",
    "        X_val_fold, y_val_fold = X[val_idx], y[val_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        tile_preds = model.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "        df_val = pd.DataFrame({\n",
    "            \"patient\": patients[val_idx],\n",
    "            \"pred\": tile_preds,\n",
    "            \"y\": y_val_fold\n",
    "        })\n",
    "        \n",
    "        # agregation tile / patient \n",
    "        patient_pred = df_val.groupby(\"patient\")[\"pred\"].mean()\n",
    "        patient_y = df_val.groupby(\"patient\")[\"y\"].first()\n",
    "\n",
    "        auc = roc_auc_score(patient_y, patient_pred)\n",
    "        aucs.append(auc)\n",
    "\n",
    "        print(f\"Fold {fold+1} AUC: {auc:.3f}\")\n",
    "\n",
    "    print(f\"\\nMean AUC: {np.mean(aucs):.3f}\")\n",
    "    return np.mean(aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    C=0.001,\n",
    "    solver=\"liblinear\",\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=GLOBAL_SEED,\n",
    ")\n",
    "\n",
    "mean_auc, std_auc = run_5fold_cv(lr, X_train, y_train,\n",
    "                           patients_train,\n",
    "                           patients_unique,\n",
    "                           y_unique)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgb = HistGradientBoostingClassifier(\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=200,\n",
    "    random_state=GLOBAL_SEED,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_auc, std_auc = run_5fold_cv(hgb, X_train, y_train,\n",
    "                           patients_train,\n",
    "                           patients_unique,\n",
    "                           y_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_cv(model, X_train, y_train, patients_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## + Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=2048, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X_train, y_train, X_val, y_val,\n",
    "              input_dim,\n",
    "              hidden_dim=128,\n",
    "              lr=1e-3,\n",
    "              epochs=EPOCHS,\n",
    "              batch_size=BATCH_SIZE):\n",
    "\n",
    "    Xtr = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    ytr = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    Xval = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    yval = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "    model = MLP(input_dim=input_dim, hidden_dim=hidden_dim).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        perm = torch.randperm(Xtr.size(0))\n",
    "\n",
    "        for i in range(0, Xtr.size(0), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = Xtr[idx]\n",
    "            yb = ytr[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_logits = model(Xval)\n",
    "        preds_val = torch.sigmoid(val_logits).cpu().numpy().ravel()\n",
    "\n",
    "    return preds_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucs = []\n",
    "lrs = []\n",
    "\n",
    "for k in range(5):\n",
    "    kfold = StratifiedKFold(5, shuffle=True, random_state=k)\n",
    "    fold = 0\n",
    "\n",
    "    for train_idx_, val_idx_ in kfold.split(patients_unique, y_unique):\n",
    "\n",
    "        # tile indexes for patients in fold\n",
    "        train_idx = np.arange(len(X_train))[\n",
    "            pd.Series(patients_train).isin(patients_unique[train_idx_])\n",
    "        ]\n",
    "        val_idx = np.arange(len(X_train))[\n",
    "            pd.Series(patients_train).isin(patients_unique[val_idx_])\n",
    "        ]\n",
    "\n",
    "        # folds\n",
    "        X_fold_train = X_train[train_idx]\n",
    "        y_fold_train = y_train[train_idx]\n",
    "        X_fold_val = X_train[val_idx]\n",
    "        y_fold_val = y_train[val_idx]\n",
    "\n",
    "        scaler = StandardScaler().fit(X_fold_train)\n",
    "        X_fold_train = scaler.transform(X_fold_train)\n",
    "        X_fold_val = scaler.transform(X_fold_val)\n",
    "\n",
    "        preds_val = train_mlp(\n",
    "            X_fold_train,\n",
    "            y_fold_train,\n",
    "            X_fold_val,\n",
    "            y_fold_val,\n",
    "            input_dim=X_fold_train.shape[1],\n",
    "            hidden_dim=128,\n",
    "            lr=1e-3,\n",
    "            epochs=EPOCHS\n",
    "        )\n",
    "        \n",
    "        # ---- PATIENT-LEVEL AGGREGATION ----\n",
    "        val_pat = patients_train[val_idx]\n",
    "        df_val = pd.DataFrame({\n",
    "            \"patient\": val_pat,\n",
    "            \"pred\": preds_val,\n",
    "            \"y\": y_fold_val\n",
    "        })\n",
    "\n",
    "        patient_pred = df_val.groupby(\"patient\")[\"pred\"].mean()\n",
    "        patient_y = df_val.groupby(\"patient\")[\"y\"].first()\n",
    "\n",
    "        auc = roc_auc_score(patient_y, patient_pred)\n",
    "\n",
    "        print(f\"AUC on split {k} fold {fold}: {auc:.3f}\")\n",
    "        aucs.append(auc)\n",
    "        lrs.append(lr)\n",
    "\n",
    "        fold += 1\n",
    "\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "print(\n",
    "    f\"5-fold cross-validated PATIENT-level AUC averaged over {k+1} repeats: \"\n",
    "    f\"{np.mean(aucs):.3f} ({np.std(aucs):.3f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the previous models trained through cross-validation so that to produce a submission file that can directly be uploaded on the data challenge platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:54.987815Z",
     "start_time": "2022-10-24T07:22:54.079916Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test_tiles = []\n",
    "sample_ids_test_tiles = []\n",
    "\n",
    "# keep test preprocessing at tile-level with deterministic cap\n",
    "max_test_tiles_per_sample = MAX_TILES_PER_PATIENT if \"MAX_TILES_PER_PATIENT\" in globals() else None\n",
    "rng_test = np.random.RandomState(GLOBAL_SEED)\n",
    "\n",
    "# load the test data from `df_test` (~ 1 minute)\n",
    "for sample in tqdm(df_test[\"Sample ID\"].values):\n",
    "    _features = np.load(test_features_dir / sample)\n",
    "    features = _features[:, 3:]\n",
    "\n",
    "    if max_test_tiles_per_sample is not None and len(features) > max_test_tiles_per_sample:\n",
    "        keep = rng_test.choice(len(features), max_test_tiles_per_sample, replace=False)\n",
    "        features = features[keep]\n",
    "\n",
    "    X_test_tiles.append(features)\n",
    "    sample_ids_test_tiles.append(np.full(len(features), sample))\n",
    "\n",
    "X_test_tiles = np.vstack(X_test_tiles)\n",
    "sample_ids_test_tiles = np.concatenate(sample_ids_test_tiles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T08:17:35.617554Z",
     "start_time": "2022-10-20T08:17:35.603562Z"
    }
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:55.043255Z",
     "start_time": "2022-10-24T07:22:54.989274Z"
    }
   },
   "outputs": [],
   "source": [
    "preds_test = []\n",
    "\n",
    "for sample in df_test[\"Sample ID\"].values:\n",
    "    _features = np.load(test_features_dir / sample)\n",
    "    features = _features[:, 3:]\n",
    "\n",
    "    tile_preds = final_model.predict_proba(features)[:, 1]\n",
    "    slide_pred = np.mean(tile_preds)\n",
    "\n",
    "    preds_test.append(slide_pred)\n",
    "\n",
    "preds_test = np.array(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:22:55.098571Z",
     "start_time": "2022-10-24T07:22:55.044975Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(\n",
    "    {\"Sample ID\": df_test[\"Sample ID\"].values, \"Target\": preds_test}\n",
    ").sort_values(\n",
    "    \"Sample ID\"\n",
    ")  # extra step to sort the sample IDs\n",
    "\n",
    "# sanity checks\n",
    "assert submission[\"Target\"].notna().all(), \"Some samples have missing predictions\"\n",
    "assert all(submission[\"Target\"].between(0, 1)), \"`Target` values must be in [0, 1]\"\n",
    "assert submission.shape == (len(df_test), 2), \"Your submission file must match test size\"\n",
    "assert submission.shape == (149, 2), \"Your submission file must be of shape (149, 2)\"\n",
    "assert list(submission.columns) == [\n",
    "    \"Sample ID\",\n",
    "    \"Target\",\n",
    "], \"Your submission file must have columns `Sample ID` and `Target`\"\n",
    "\n",
    "# save the submission as a csv file\n",
    "submission.to_csv(data_dir / \"output\" / \"Random_Forest_tile_level.csv\", index=None)\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code aims to load and manipulate the images provided as part of  this challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scanning images paths on disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can take up to 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:00.263580Z",
     "start_time": "2022-10-24T07:22:55.100342Z"
    }
   },
   "outputs": [],
   "source": [
    "train_images_dir = data_dir / \"train_input\" / \"images\"\n",
    "train_images_files = list(train_images_dir.rglob(\"*.jpg\"))\n",
    "\n",
    "test_images_dir = data_dir / \"test_input\" / \"images\"\n",
    "test_images_files = list(test_images_dir.rglob(\"*.jpg\"))\n",
    "\n",
    "print(\n",
    "    f\"Number of images\\n\"\n",
    "    \"-----------------\\n\"\n",
    "    f\"Train: {len(train_images_files)}\\n\" # 344 x 1000 = 344,000 tiles\n",
    "    f\"Test: {len(test_images_files)}\\n\"  # 149 x 1000 = 149,000 tiles\n",
    "    f\"Total: {len(train_images_files) + len(test_images_files)}\\n\"  # 493 x 1000 = 493,000 tiles\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-20T10:16:48.078600Z",
     "start_time": "2022-10-20T10:16:47.948127Z"
    }
   },
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load some of the `.jpg` images for a given sample, say `ID_001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:00.381225Z",
     "start_time": "2022-10-24T07:23:00.267047Z"
    }
   },
   "outputs": [],
   "source": [
    "ID_001_tiles = [p for p in train_images_files if 'ID_001' in p.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.973155Z",
     "start_time": "2022-10-24T07:23:00.382760Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 5)\n",
    "fig.set_size_inches(12, 12)\n",
    "\n",
    "for i, img_file in enumerate(ID_001_tiles[:25]):\n",
    "    # get the metadata from the file path\n",
    "    _, metadata = str(img_file).split(\"tile_\")\n",
    "    id_tile, level, x, y = metadata[:-4].split(\"_\")\n",
    "    img = plt.imread(img_file)\n",
    "    ax = axes[i//5, i%5]\n",
    "    ax.imshow(img)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"Tile {id_tile} ({x}, {y})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping with features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the coordinates in the features matrices and tiles number are aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.984327Z",
     "start_time": "2022-10-24T07:23:01.974933Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = \"ID_001.npy\"\n",
    "_features = np.load(train_features_dir / sample)\n",
    "coordinates, features = _features[:, :3], _features[:, 3:]\n",
    "print(\"xy features coordinates\")\n",
    "coordinates[:10, 1:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-24T07:23:01.990342Z",
     "start_time": "2022-10-24T07:23:01.985926Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Tiles numbering and features coordinates\\n\"\n",
    ")\n",
    "[tile.name for tile in ID_001_tiles[:10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
